{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from numba.core.errors import NumbaDeprecationWarning\n",
    "warnings.filterwarnings('ignore',category=NumbaDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SIReNet.epidemics_graph import EpidemicsGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mrcharles/SIReNet_project\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = EpidemicsGraph(pop_size=10000)\n",
    "city.sampler_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "city.create_families()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x10000 sparse matrix of type '<class 'numpy.bool_'>'\n",
       "\twith 17958 stored elements in Dictionary Of Keys format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city.adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "city.add_links(get_richer_step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "city.build_nx_graph()\n",
    "city.degree_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9926, 3, 2, 2, 2, 2, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city.components_dimension()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x10000 sparse matrix of type '<class 'numpy.int16'>'\n",
       "\twith 646230 stored elements in Dictionary Of Keys format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city.compute_common_neighbors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "city.initialize_individual_factors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487 people have been infected\n"
     ]
    }
   ],
   "source": [
    "city.start_infection(contagion_probability=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-13-093f9495d45c>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-093f9495d45c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    city.propagate_infection(mu=2,time)\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    city.propagate_infection(mu=2,time)\n",
    "\n",
    "city.propagation_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city.export_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from pygcn.utils import load_data, accuracy\n",
    "from pygcn.models import GCN\n",
    "from pygcn.utils import normalize\n",
    "from pygcn.utils import  sparse_mx_to_torch_sparse_tensor\n",
    "\n",
    "from SIReNet.utils import  IndexSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2070'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.595832551154496\n",
      "5327\n"
     ]
    }
   ],
   "source": [
    "#load exported data\n",
    "adj = sp.load_npz('adjacency.npz')\n",
    "adj = normalize(adj)\n",
    "\n",
    "features = pd.read_csv('node_features.csv')\n",
    "indexes = features.index.values\n",
    "labels = features['labels'].values\n",
    "\n",
    "print(np.sum(labels)/labels.shape[0])\n",
    "\n",
    "features = features.drop(['labels'],axis=1)\n",
    "feature_transformer =  StandardScaler()\n",
    "features = feature_transformer.fit_transform(features)\n",
    "\n",
    "features = torch.FloatTensor(features)\n",
    "labels = torch.LongTensor(labels)\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "\n",
    "print(len(indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3327\n",
      "2327\n"
     ]
    }
   ],
   "source": [
    "#indexes\n",
    "idx_sampler = IndexSampler(indexes)\n",
    "idx_train = torch.LongTensor(idx_sampler.sample(n_samples=2000))\n",
    "idx_val = torch.LongTensor(idx_sampler.sample(n_samples=1000))\n",
    "idx_test = torch.LongTensor(idx_sampler.sample_remaining())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5327, 5327])\n",
      "torch.Size([5327, 3])\n",
      "torch.Size([5327])\n",
      "torch.Size([2000])\n",
      "torch.Size([1000])\n",
      "torch.Size([2327])\n"
     ]
    }
   ],
   "source": [
    "print(adj.shape)\n",
    "print(features.shape)\n",
    "print(labels.shape)\n",
    "print(idx_train.shape)\n",
    "print(idx_val.shape)\n",
    "print(idx_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=16,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0.5)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move tensors to cuda\n",
    "model.cuda()\n",
    "features = features.cuda()\n",
    "adj = adj.cuda()\n",
    "labels = labels.cuda()\n",
    "idx_train = idx_train.cuda()\n",
    "idx_val = idx_val.cuda()\n",
    "idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.8819 acc_train: 0.4140 loss_val: 0.8764 acc_val: 0.3880 time: 0.3402s\n",
      "Epoch: 0002 loss_train: 0.8556 acc_train: 0.4140 loss_val: 0.8506 acc_val: 0.3880 time: 0.0031s\n",
      "Epoch: 0003 loss_train: 0.8294 acc_train: 0.4140 loss_val: 0.8262 acc_val: 0.3880 time: 0.0030s\n",
      "Epoch: 0004 loss_train: 0.8112 acc_train: 0.4135 loss_val: 0.8031 acc_val: 0.3880 time: 0.0028s\n",
      "Epoch: 0005 loss_train: 0.7897 acc_train: 0.4140 loss_val: 0.7813 acc_val: 0.3880 time: 0.0030s\n",
      "Epoch: 0006 loss_train: 0.7701 acc_train: 0.4130 loss_val: 0.7606 acc_val: 0.3880 time: 0.0030s\n",
      "Epoch: 0007 loss_train: 0.7545 acc_train: 0.4125 loss_val: 0.7410 acc_val: 0.3880 time: 0.0029s\n",
      "Epoch: 0008 loss_train: 0.7377 acc_train: 0.4165 loss_val: 0.7224 acc_val: 0.3880 time: 0.0030s\n",
      "Epoch: 0009 loss_train: 0.7163 acc_train: 0.4265 loss_val: 0.7048 acc_val: 0.3880 time: 0.0030s\n",
      "Epoch: 0010 loss_train: 0.7055 acc_train: 0.4450 loss_val: 0.6882 acc_val: 0.3950 time: 0.0030s\n",
      "Epoch: 0011 loss_train: 0.6918 acc_train: 0.4810 loss_val: 0.6725 acc_val: 0.5010 time: 0.0028s\n",
      "Epoch: 0012 loss_train: 0.6794 acc_train: 0.5210 loss_val: 0.6577 acc_val: 0.6400 time: 0.0029s\n",
      "Epoch: 0013 loss_train: 0.6638 acc_train: 0.5925 loss_val: 0.6439 acc_val: 0.6970 time: 0.0031s\n",
      "Epoch: 0014 loss_train: 0.6546 acc_train: 0.6380 loss_val: 0.6311 acc_val: 0.7310 time: 0.0030s\n",
      "Epoch: 0015 loss_train: 0.6456 acc_train: 0.6495 loss_val: 0.6193 acc_val: 0.7440 time: 0.0030s\n",
      "Epoch: 0016 loss_train: 0.6306 acc_train: 0.6855 loss_val: 0.6083 acc_val: 0.7390 time: 0.0028s\n",
      "Epoch: 0017 loss_train: 0.6260 acc_train: 0.6910 loss_val: 0.5982 acc_val: 0.7380 time: 0.0030s\n",
      "Epoch: 0018 loss_train: 0.6172 acc_train: 0.6895 loss_val: 0.5890 acc_val: 0.7330 time: 0.0030s\n",
      "Epoch: 0019 loss_train: 0.6145 acc_train: 0.7050 loss_val: 0.5806 acc_val: 0.7340 time: 0.0030s\n",
      "Epoch: 0020 loss_train: 0.6082 acc_train: 0.6945 loss_val: 0.5728 acc_val: 0.7360 time: 0.0030s\n",
      "Epoch: 0021 loss_train: 0.6011 acc_train: 0.7110 loss_val: 0.5657 acc_val: 0.7400 time: 0.0030s\n",
      "Epoch: 0022 loss_train: 0.5917 acc_train: 0.7095 loss_val: 0.5591 acc_val: 0.7400 time: 0.0030s\n",
      "Epoch: 0023 loss_train: 0.5897 acc_train: 0.7200 loss_val: 0.5530 acc_val: 0.7400 time: 0.0031s\n",
      "Epoch: 0024 loss_train: 0.5783 acc_train: 0.7225 loss_val: 0.5472 acc_val: 0.7450 time: 0.0030s\n",
      "Epoch: 0025 loss_train: 0.5876 acc_train: 0.7155 loss_val: 0.5420 acc_val: 0.7480 time: 0.0030s\n",
      "Epoch: 0026 loss_train: 0.5756 acc_train: 0.7180 loss_val: 0.5373 acc_val: 0.7550 time: 0.0030s\n",
      "Epoch: 0027 loss_train: 0.5727 acc_train: 0.7175 loss_val: 0.5332 acc_val: 0.7600 time: 0.0030s\n",
      "Epoch: 0028 loss_train: 0.5711 acc_train: 0.7270 loss_val: 0.5298 acc_val: 0.7580 time: 0.0030s\n",
      "Epoch: 0029 loss_train: 0.5661 acc_train: 0.7310 loss_val: 0.5271 acc_val: 0.7560 time: 0.0030s\n",
      "Epoch: 0030 loss_train: 0.5598 acc_train: 0.7380 loss_val: 0.5249 acc_val: 0.7570 time: 0.0030s\n",
      "Epoch: 0031 loss_train: 0.5578 acc_train: 0.7340 loss_val: 0.5231 acc_val: 0.7530 time: 0.0030s\n",
      "Epoch: 0032 loss_train: 0.5646 acc_train: 0.7295 loss_val: 0.5215 acc_val: 0.7540 time: 0.0030s\n",
      "Epoch: 0033 loss_train: 0.5652 acc_train: 0.7280 loss_val: 0.5201 acc_val: 0.7540 time: 0.0031s\n",
      "Epoch: 0034 loss_train: 0.5557 acc_train: 0.7295 loss_val: 0.5186 acc_val: 0.7570 time: 0.0030s\n",
      "Epoch: 0035 loss_train: 0.5680 acc_train: 0.7400 loss_val: 0.5170 acc_val: 0.7560 time: 0.0030s\n",
      "Epoch: 0036 loss_train: 0.5600 acc_train: 0.7430 loss_val: 0.5154 acc_val: 0.7590 time: 0.0030s\n",
      "Epoch: 0037 loss_train: 0.5618 acc_train: 0.7380 loss_val: 0.5139 acc_val: 0.7590 time: 0.0030s\n",
      "Epoch: 0038 loss_train: 0.5483 acc_train: 0.7345 loss_val: 0.5126 acc_val: 0.7590 time: 0.0030s\n",
      "Epoch: 0039 loss_train: 0.5673 acc_train: 0.7320 loss_val: 0.5116 acc_val: 0.7630 time: 0.0030s\n",
      "Epoch: 0040 loss_train: 0.5546 acc_train: 0.7410 loss_val: 0.5106 acc_val: 0.7640 time: 0.0030s\n",
      "Epoch: 0041 loss_train: 0.5485 acc_train: 0.7420 loss_val: 0.5098 acc_val: 0.7670 time: 0.0030s\n",
      "Epoch: 0042 loss_train: 0.5398 acc_train: 0.7520 loss_val: 0.5091 acc_val: 0.7700 time: 0.0030s\n",
      "Epoch: 0043 loss_train: 0.5449 acc_train: 0.7420 loss_val: 0.5085 acc_val: 0.7690 time: 0.0030s\n",
      "Epoch: 0044 loss_train: 0.5477 acc_train: 0.7515 loss_val: 0.5080 acc_val: 0.7680 time: 0.0028s\n",
      "Epoch: 0045 loss_train: 0.5407 acc_train: 0.7515 loss_val: 0.5074 acc_val: 0.7690 time: 0.0030s\n",
      "Epoch: 0046 loss_train: 0.5525 acc_train: 0.7400 loss_val: 0.5070 acc_val: 0.7720 time: 0.0030s\n",
      "Epoch: 0047 loss_train: 0.5576 acc_train: 0.7500 loss_val: 0.5067 acc_val: 0.7720 time: 0.0030s\n",
      "Epoch: 0048 loss_train: 0.5532 acc_train: 0.7435 loss_val: 0.5065 acc_val: 0.7720 time: 0.0030s\n",
      "Epoch: 0049 loss_train: 0.5527 acc_train: 0.7400 loss_val: 0.5064 acc_val: 0.7690 time: 0.0028s\n",
      "Epoch: 0050 loss_train: 0.5457 acc_train: 0.7480 loss_val: 0.5063 acc_val: 0.7690 time: 0.0028s\n",
      "Epoch: 0051 loss_train: 0.5410 acc_train: 0.7485 loss_val: 0.5061 acc_val: 0.7680 time: 0.0028s\n",
      "Epoch: 0052 loss_train: 0.5422 acc_train: 0.7525 loss_val: 0.5060 acc_val: 0.7680 time: 0.0030s\n",
      "Epoch: 0053 loss_train: 0.5498 acc_train: 0.7450 loss_val: 0.5058 acc_val: 0.7680 time: 0.0030s\n",
      "Epoch: 0054 loss_train: 0.5328 acc_train: 0.7555 loss_val: 0.5057 acc_val: 0.7680 time: 0.0030s\n",
      "Epoch: 0055 loss_train: 0.5330 acc_train: 0.7600 loss_val: 0.5056 acc_val: 0.7680 time: 0.0030s\n",
      "Epoch: 0056 loss_train: 0.5418 acc_train: 0.7510 loss_val: 0.5055 acc_val: 0.7700 time: 0.0030s\n",
      "Epoch: 0057 loss_train: 0.5359 acc_train: 0.7505 loss_val: 0.5054 acc_val: 0.7710 time: 0.0031s\n",
      "Epoch: 0058 loss_train: 0.5337 acc_train: 0.7655 loss_val: 0.5052 acc_val: 0.7710 time: 0.0030s\n",
      "Epoch: 0059 loss_train: 0.5322 acc_train: 0.7610 loss_val: 0.5050 acc_val: 0.7720 time: 0.0030s\n",
      "Epoch: 0060 loss_train: 0.5404 acc_train: 0.7630 loss_val: 0.5047 acc_val: 0.7720 time: 0.0030s\n",
      "Epoch: 0061 loss_train: 0.5407 acc_train: 0.7580 loss_val: 0.5044 acc_val: 0.7740 time: 0.0031s\n",
      "Epoch: 0062 loss_train: 0.5321 acc_train: 0.7715 loss_val: 0.5041 acc_val: 0.7730 time: 0.0030s\n",
      "Epoch: 0063 loss_train: 0.5294 acc_train: 0.7505 loss_val: 0.5039 acc_val: 0.7730 time: 0.0030s\n",
      "Epoch: 0064 loss_train: 0.5396 acc_train: 0.7605 loss_val: 0.5036 acc_val: 0.7730 time: 0.0030s\n",
      "Epoch: 0065 loss_train: 0.5274 acc_train: 0.7695 loss_val: 0.5035 acc_val: 0.7740 time: 0.0030s\n",
      "Epoch: 0066 loss_train: 0.5298 acc_train: 0.7530 loss_val: 0.5033 acc_val: 0.7750 time: 0.0031s\n",
      "Epoch: 0067 loss_train: 0.5330 acc_train: 0.7550 loss_val: 0.5030 acc_val: 0.7760 time: 0.0030s\n",
      "Epoch: 0068 loss_train: 0.5473 acc_train: 0.7640 loss_val: 0.5028 acc_val: 0.7760 time: 0.0031s\n",
      "Epoch: 0069 loss_train: 0.5320 acc_train: 0.7570 loss_val: 0.5025 acc_val: 0.7750 time: 0.0031s\n",
      "Epoch: 0070 loss_train: 0.5245 acc_train: 0.7705 loss_val: 0.5023 acc_val: 0.7750 time: 0.0030s\n",
      "Epoch: 0071 loss_train: 0.5292 acc_train: 0.7605 loss_val: 0.5021 acc_val: 0.7770 time: 0.0031s\n",
      "Epoch: 0072 loss_train: 0.5259 acc_train: 0.7555 loss_val: 0.5018 acc_val: 0.7780 time: 0.0028s\n",
      "Epoch: 0073 loss_train: 0.5284 acc_train: 0.7560 loss_val: 0.5015 acc_val: 0.7780 time: 0.0029s\n",
      "Epoch: 0074 loss_train: 0.5261 acc_train: 0.7640 loss_val: 0.5011 acc_val: 0.7770 time: 0.0031s\n",
      "Epoch: 0075 loss_train: 0.5317 acc_train: 0.7555 loss_val: 0.5008 acc_val: 0.7780 time: 0.0028s\n",
      "Epoch: 0076 loss_train: 0.5340 acc_train: 0.7555 loss_val: 0.5006 acc_val: 0.7800 time: 0.0030s\n",
      "Epoch: 0077 loss_train: 0.5242 acc_train: 0.7680 loss_val: 0.5003 acc_val: 0.7800 time: 0.0030s\n",
      "Epoch: 0078 loss_train: 0.5258 acc_train: 0.7640 loss_val: 0.4999 acc_val: 0.7800 time: 0.0030s\n",
      "Epoch: 0079 loss_train: 0.5213 acc_train: 0.7730 loss_val: 0.4997 acc_val: 0.7790 time: 0.0030s\n",
      "Epoch: 0080 loss_train: 0.5251 acc_train: 0.7675 loss_val: 0.4994 acc_val: 0.7780 time: 0.0030s\n",
      "Epoch: 0081 loss_train: 0.5225 acc_train: 0.7735 loss_val: 0.4991 acc_val: 0.7790 time: 0.0030s\n",
      "Epoch: 0082 loss_train: 0.5274 acc_train: 0.7720 loss_val: 0.4987 acc_val: 0.7790 time: 0.0030s\n",
      "Epoch: 0083 loss_train: 0.5217 acc_train: 0.7705 loss_val: 0.4983 acc_val: 0.7780 time: 0.0030s\n",
      "Epoch: 0084 loss_train: 0.5305 acc_train: 0.7640 loss_val: 0.4980 acc_val: 0.7780 time: 0.0028s\n",
      "Epoch: 0085 loss_train: 0.5202 acc_train: 0.7685 loss_val: 0.4977 acc_val: 0.7780 time: 0.0030s\n",
      "Epoch: 0086 loss_train: 0.5273 acc_train: 0.7620 loss_val: 0.4974 acc_val: 0.7790 time: 0.0030s\n",
      "Epoch: 0087 loss_train: 0.5289 acc_train: 0.7780 loss_val: 0.4971 acc_val: 0.7800 time: 0.0030s\n",
      "Epoch: 0088 loss_train: 0.5260 acc_train: 0.7615 loss_val: 0.4969 acc_val: 0.7850 time: 0.0030s\n",
      "Epoch: 0089 loss_train: 0.5279 acc_train: 0.7705 loss_val: 0.4968 acc_val: 0.7840 time: 0.0030s\n",
      "Epoch: 0090 loss_train: 0.5310 acc_train: 0.7595 loss_val: 0.4967 acc_val: 0.7840 time: 0.0030s\n",
      "Epoch: 0091 loss_train: 0.5154 acc_train: 0.7780 loss_val: 0.4965 acc_val: 0.7860 time: 0.0030s\n",
      "Epoch: 0092 loss_train: 0.5182 acc_train: 0.7755 loss_val: 0.4962 acc_val: 0.7860 time: 0.0030s\n",
      "Epoch: 0093 loss_train: 0.5182 acc_train: 0.7700 loss_val: 0.4959 acc_val: 0.7850 time: 0.0030s\n",
      "Epoch: 0094 loss_train: 0.5210 acc_train: 0.7765 loss_val: 0.4956 acc_val: 0.7850 time: 0.0029s\n",
      "Epoch: 0095 loss_train: 0.5196 acc_train: 0.7720 loss_val: 0.4953 acc_val: 0.7870 time: 0.0030s\n",
      "Epoch: 0096 loss_train: 0.5240 acc_train: 0.7610 loss_val: 0.4951 acc_val: 0.7860 time: 0.0028s\n",
      "Epoch: 0097 loss_train: 0.5247 acc_train: 0.7730 loss_val: 0.4949 acc_val: 0.7870 time: 0.0030s\n",
      "Epoch: 0098 loss_train: 0.5160 acc_train: 0.7760 loss_val: 0.4947 acc_val: 0.7860 time: 0.0030s\n",
      "Epoch: 0099 loss_train: 0.5142 acc_train: 0.7765 loss_val: 0.4946 acc_val: 0.7850 time: 0.0030s\n",
      "Epoch: 0100 loss_train: 0.5150 acc_train: 0.7770 loss_val: 0.4945 acc_val: 0.7850 time: 0.0030s\n",
      "Epoch: 0101 loss_train: 0.5215 acc_train: 0.7650 loss_val: 0.4943 acc_val: 0.7880 time: 0.0028s\n",
      "Epoch: 0102 loss_train: 0.5296 acc_train: 0.7695 loss_val: 0.4943 acc_val: 0.7870 time: 0.0030s\n",
      "Epoch: 0103 loss_train: 0.5096 acc_train: 0.7695 loss_val: 0.4941 acc_val: 0.7860 time: 0.0030s\n",
      "Epoch: 0104 loss_train: 0.5157 acc_train: 0.7785 loss_val: 0.4940 acc_val: 0.7860 time: 0.0030s\n",
      "Epoch: 0105 loss_train: 0.5135 acc_train: 0.7715 loss_val: 0.4938 acc_val: 0.7860 time: 0.0030s\n",
      "Epoch: 0106 loss_train: 0.5152 acc_train: 0.7745 loss_val: 0.4935 acc_val: 0.7870 time: 0.0030s\n",
      "Epoch: 0107 loss_train: 0.5115 acc_train: 0.7810 loss_val: 0.4933 acc_val: 0.7880 time: 0.0030s\n",
      "Epoch: 0108 loss_train: 0.5110 acc_train: 0.7740 loss_val: 0.4932 acc_val: 0.7890 time: 0.0030s\n",
      "Epoch: 0109 loss_train: 0.5185 acc_train: 0.7665 loss_val: 0.4930 acc_val: 0.7890 time: 0.0030s\n",
      "Epoch: 0110 loss_train: 0.5123 acc_train: 0.7775 loss_val: 0.4927 acc_val: 0.7890 time: 0.0030s\n",
      "Epoch: 0111 loss_train: 0.5190 acc_train: 0.7735 loss_val: 0.4924 acc_val: 0.7890 time: 0.0030s\n",
      "Epoch: 0112 loss_train: 0.5127 acc_train: 0.7820 loss_val: 0.4922 acc_val: 0.7900 time: 0.0030s\n",
      "Epoch: 0113 loss_train: 0.5101 acc_train: 0.7845 loss_val: 0.4920 acc_val: 0.7910 time: 0.0030s\n",
      "Epoch: 0114 loss_train: 0.5097 acc_train: 0.7745 loss_val: 0.4918 acc_val: 0.7910 time: 0.0030s\n",
      "Epoch: 0115 loss_train: 0.5135 acc_train: 0.7715 loss_val: 0.4917 acc_val: 0.7900 time: 0.0029s\n",
      "Epoch: 0116 loss_train: 0.5177 acc_train: 0.7705 loss_val: 0.4916 acc_val: 0.7910 time: 0.0030s\n",
      "Epoch: 0117 loss_train: 0.5193 acc_train: 0.7825 loss_val: 0.4915 acc_val: 0.7890 time: 0.0030s\n",
      "Epoch: 0118 loss_train: 0.5029 acc_train: 0.7750 loss_val: 0.4914 acc_val: 0.7900 time: 0.0030s\n",
      "Epoch: 0119 loss_train: 0.5105 acc_train: 0.7740 loss_val: 0.4912 acc_val: 0.7900 time: 0.0028s\n",
      "Epoch: 0120 loss_train: 0.5117 acc_train: 0.7695 loss_val: 0.4910 acc_val: 0.7900 time: 0.0030s\n",
      "Epoch: 0121 loss_train: 0.5077 acc_train: 0.7770 loss_val: 0.4908 acc_val: 0.7910 time: 0.0028s\n",
      "Epoch: 0122 loss_train: 0.5087 acc_train: 0.7860 loss_val: 0.4906 acc_val: 0.7920 time: 0.0028s\n",
      "Epoch: 0123 loss_train: 0.5111 acc_train: 0.7775 loss_val: 0.4905 acc_val: 0.7920 time: 0.0030s\n",
      "Epoch: 0124 loss_train: 0.5061 acc_train: 0.7805 loss_val: 0.4903 acc_val: 0.7910 time: 0.0028s\n",
      "Epoch: 0125 loss_train: 0.5070 acc_train: 0.7720 loss_val: 0.4901 acc_val: 0.7910 time: 0.0029s\n",
      "Epoch: 0126 loss_train: 0.5071 acc_train: 0.7795 loss_val: 0.4900 acc_val: 0.7900 time: 0.0029s\n",
      "Epoch: 0127 loss_train: 0.5066 acc_train: 0.7800 loss_val: 0.4898 acc_val: 0.7910 time: 0.0030s\n",
      "Epoch: 0128 loss_train: 0.5126 acc_train: 0.7740 loss_val: 0.4895 acc_val: 0.7910 time: 0.0028s\n",
      "Epoch: 0129 loss_train: 0.5124 acc_train: 0.7630 loss_val: 0.4893 acc_val: 0.7910 time: 0.0030s\n",
      "Epoch: 0130 loss_train: 0.5046 acc_train: 0.7850 loss_val: 0.4890 acc_val: 0.7930 time: 0.0030s\n",
      "Epoch: 0131 loss_train: 0.5147 acc_train: 0.7705 loss_val: 0.4888 acc_val: 0.7930 time: 0.0030s\n",
      "Epoch: 0132 loss_train: 0.5128 acc_train: 0.7690 loss_val: 0.4887 acc_val: 0.7930 time: 0.0030s\n",
      "Epoch: 0133 loss_train: 0.5080 acc_train: 0.7820 loss_val: 0.4886 acc_val: 0.7930 time: 0.0030s\n",
      "Epoch: 0134 loss_train: 0.5039 acc_train: 0.7780 loss_val: 0.4885 acc_val: 0.7940 time: 0.0030s\n",
      "Epoch: 0135 loss_train: 0.5120 acc_train: 0.7805 loss_val: 0.4884 acc_val: 0.7940 time: 0.0030s\n",
      "Epoch: 0136 loss_train: 0.5141 acc_train: 0.7880 loss_val: 0.4883 acc_val: 0.7940 time: 0.0030s\n",
      "Epoch: 0137 loss_train: 0.4994 acc_train: 0.7790 loss_val: 0.4882 acc_val: 0.7940 time: 0.0030s\n",
      "Epoch: 0138 loss_train: 0.5152 acc_train: 0.7775 loss_val: 0.4881 acc_val: 0.7940 time: 0.0030s\n",
      "Epoch: 0139 loss_train: 0.5036 acc_train: 0.7835 loss_val: 0.4880 acc_val: 0.7940 time: 0.0028s\n",
      "Epoch: 0140 loss_train: 0.5018 acc_train: 0.7825 loss_val: 0.4879 acc_val: 0.7960 time: 0.0030s\n",
      "Epoch: 0141 loss_train: 0.5067 acc_train: 0.7835 loss_val: 0.4879 acc_val: 0.7960 time: 0.0028s\n",
      "Epoch: 0142 loss_train: 0.5073 acc_train: 0.7830 loss_val: 0.4878 acc_val: 0.7960 time: 0.0030s\n",
      "Epoch: 0143 loss_train: 0.5112 acc_train: 0.7805 loss_val: 0.4877 acc_val: 0.7960 time: 0.0030s\n",
      "Epoch: 0144 loss_train: 0.5103 acc_train: 0.7830 loss_val: 0.4875 acc_val: 0.7950 time: 0.0030s\n",
      "Epoch: 0145 loss_train: 0.5141 acc_train: 0.7885 loss_val: 0.4874 acc_val: 0.7960 time: 0.0030s\n",
      "Epoch: 0146 loss_train: 0.5098 acc_train: 0.7745 loss_val: 0.4873 acc_val: 0.7970 time: 0.0030s\n",
      "Epoch: 0147 loss_train: 0.4997 acc_train: 0.7940 loss_val: 0.4872 acc_val: 0.7960 time: 0.0031s\n",
      "Epoch: 0148 loss_train: 0.5056 acc_train: 0.7785 loss_val: 0.4872 acc_val: 0.7960 time: 0.0030s\n",
      "Epoch: 0149 loss_train: 0.5067 acc_train: 0.7825 loss_val: 0.4872 acc_val: 0.7970 time: 0.0030s\n",
      "Epoch: 0150 loss_train: 0.5051 acc_train: 0.7815 loss_val: 0.4871 acc_val: 0.7970 time: 0.0030s\n",
      "Epoch: 0151 loss_train: 0.5056 acc_train: 0.7890 loss_val: 0.4870 acc_val: 0.7970 time: 0.0030s\n",
      "Epoch: 0152 loss_train: 0.5043 acc_train: 0.7915 loss_val: 0.4870 acc_val: 0.7970 time: 0.0030s\n",
      "Epoch: 0153 loss_train: 0.4969 acc_train: 0.7845 loss_val: 0.4869 acc_val: 0.7970 time: 0.0030s\n",
      "Epoch: 0154 loss_train: 0.5053 acc_train: 0.7825 loss_val: 0.4868 acc_val: 0.7970 time: 0.0028s\n",
      "Epoch: 0155 loss_train: 0.5036 acc_train: 0.7875 loss_val: 0.4867 acc_val: 0.7970 time: 0.0030s\n",
      "Epoch: 0156 loss_train: 0.5091 acc_train: 0.7880 loss_val: 0.4866 acc_val: 0.7970 time: 0.0030s\n",
      "Epoch: 0157 loss_train: 0.5047 acc_train: 0.7900 loss_val: 0.4865 acc_val: 0.7970 time: 0.0030s\n",
      "Epoch: 0158 loss_train: 0.5060 acc_train: 0.7870 loss_val: 0.4865 acc_val: 0.7970 time: 0.0030s\n",
      "Epoch: 0159 loss_train: 0.5135 acc_train: 0.7810 loss_val: 0.4864 acc_val: 0.7970 time: 0.0030s\n",
      "Epoch: 0160 loss_train: 0.5025 acc_train: 0.7895 loss_val: 0.4862 acc_val: 0.7980 time: 0.0030s\n",
      "Epoch: 0161 loss_train: 0.5014 acc_train: 0.7950 loss_val: 0.4860 acc_val: 0.7990 time: 0.0028s\n",
      "Epoch: 0162 loss_train: 0.4925 acc_train: 0.7905 loss_val: 0.4858 acc_val: 0.8010 time: 0.0030s\n",
      "Epoch: 0163 loss_train: 0.4980 acc_train: 0.7930 loss_val: 0.4856 acc_val: 0.8010 time: 0.0030s\n",
      "Epoch: 0164 loss_train: 0.5036 acc_train: 0.7895 loss_val: 0.4854 acc_val: 0.8030 time: 0.0030s\n",
      "Epoch: 0165 loss_train: 0.4975 acc_train: 0.7850 loss_val: 0.4852 acc_val: 0.8030 time: 0.0028s\n",
      "Epoch: 0166 loss_train: 0.5084 acc_train: 0.7875 loss_val: 0.4850 acc_val: 0.8030 time: 0.0028s\n",
      "Epoch: 0167 loss_train: 0.4966 acc_train: 0.7840 loss_val: 0.4849 acc_val: 0.8040 time: 0.0030s\n",
      "Epoch: 0168 loss_train: 0.5006 acc_train: 0.7885 loss_val: 0.4849 acc_val: 0.8040 time: 0.0030s\n",
      "Epoch: 0169 loss_train: 0.5061 acc_train: 0.7840 loss_val: 0.4849 acc_val: 0.8040 time: 0.0030s\n",
      "Epoch: 0170 loss_train: 0.5023 acc_train: 0.7860 loss_val: 0.4850 acc_val: 0.8030 time: 0.0030s\n",
      "Epoch: 0171 loss_train: 0.4995 acc_train: 0.7895 loss_val: 0.4850 acc_val: 0.8030 time: 0.0030s\n",
      "Epoch: 0172 loss_train: 0.4991 acc_train: 0.7905 loss_val: 0.4851 acc_val: 0.8020 time: 0.0030s\n",
      "Epoch: 0173 loss_train: 0.5059 acc_train: 0.7915 loss_val: 0.4851 acc_val: 0.8010 time: 0.0030s\n",
      "Epoch: 0174 loss_train: 0.4977 acc_train: 0.7855 loss_val: 0.4851 acc_val: 0.8010 time: 0.0030s\n",
      "Epoch: 0175 loss_train: 0.4959 acc_train: 0.7940 loss_val: 0.4851 acc_val: 0.8010 time: 0.0030s\n",
      "Epoch: 0176 loss_train: 0.4920 acc_train: 0.7890 loss_val: 0.4850 acc_val: 0.8020 time: 0.0030s\n",
      "Epoch: 0177 loss_train: 0.4948 acc_train: 0.7925 loss_val: 0.4849 acc_val: 0.8040 time: 0.0030s\n",
      "Epoch: 0178 loss_train: 0.4996 acc_train: 0.7800 loss_val: 0.4848 acc_val: 0.8040 time: 0.0030s\n",
      "Epoch: 0179 loss_train: 0.4955 acc_train: 0.7890 loss_val: 0.4847 acc_val: 0.8040 time: 0.0030s\n",
      "Epoch: 0180 loss_train: 0.5007 acc_train: 0.7800 loss_val: 0.4845 acc_val: 0.8050 time: 0.0030s\n",
      "Epoch: 0181 loss_train: 0.4982 acc_train: 0.7955 loss_val: 0.4845 acc_val: 0.8050 time: 0.0030s\n",
      "Epoch: 0182 loss_train: 0.4972 acc_train: 0.7880 loss_val: 0.4844 acc_val: 0.8040 time: 0.0030s\n",
      "Epoch: 0183 loss_train: 0.5011 acc_train: 0.7835 loss_val: 0.4843 acc_val: 0.8040 time: 0.0030s\n",
      "Epoch: 0184 loss_train: 0.4970 acc_train: 0.7880 loss_val: 0.4842 acc_val: 0.8050 time: 0.0030s\n",
      "Epoch: 0185 loss_train: 0.4958 acc_train: 0.7870 loss_val: 0.4841 acc_val: 0.8030 time: 0.0030s\n",
      "Epoch: 0186 loss_train: 0.4975 acc_train: 0.7845 loss_val: 0.4840 acc_val: 0.8030 time: 0.0028s\n",
      "Epoch: 0187 loss_train: 0.4981 acc_train: 0.7950 loss_val: 0.4838 acc_val: 0.8030 time: 0.0030s\n",
      "Epoch: 0188 loss_train: 0.4987 acc_train: 0.7885 loss_val: 0.4838 acc_val: 0.8030 time: 0.0030s\n",
      "Epoch: 0189 loss_train: 0.4888 acc_train: 0.7905 loss_val: 0.4836 acc_val: 0.8040 time: 0.0028s\n",
      "Epoch: 0190 loss_train: 0.4938 acc_train: 0.7940 loss_val: 0.4835 acc_val: 0.8050 time: 0.0030s\n",
      "Epoch: 0191 loss_train: 0.5044 acc_train: 0.7910 loss_val: 0.4834 acc_val: 0.8050 time: 0.0030s\n",
      "Epoch: 0192 loss_train: 0.4888 acc_train: 0.7975 loss_val: 0.4832 acc_val: 0.8050 time: 0.0030s\n",
      "Epoch: 0193 loss_train: 0.5044 acc_train: 0.7810 loss_val: 0.4831 acc_val: 0.8050 time: 0.0028s\n",
      "Epoch: 0194 loss_train: 0.5025 acc_train: 0.7935 loss_val: 0.4831 acc_val: 0.8040 time: 0.0030s\n",
      "Epoch: 0195 loss_train: 0.5031 acc_train: 0.7845 loss_val: 0.4829 acc_val: 0.8050 time: 0.0028s\n",
      "Epoch: 0196 loss_train: 0.4868 acc_train: 0.7920 loss_val: 0.4828 acc_val: 0.8050 time: 0.0030s\n",
      "Epoch: 0197 loss_train: 0.4951 acc_train: 0.7865 loss_val: 0.4827 acc_val: 0.8050 time: 0.0031s\n",
      "Epoch: 0198 loss_train: 0.4993 acc_train: 0.8020 loss_val: 0.4826 acc_val: 0.8060 time: 0.0028s\n",
      "Epoch: 0199 loss_train: 0.4937 acc_train: 0.7895 loss_val: 0.4826 acc_val: 0.8060 time: 0.0030s\n",
      "Epoch: 0200 loss_train: 0.5028 acc_train: 0.7945 loss_val: 0.4827 acc_val: 0.8050 time: 0.0030s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.9453s\n",
      "Test set results: loss= 0.4666 accuracy= 0.8109\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "fastmode = False\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(200):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
